{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dade82a4-78b8-4e96-8115-b9f0b0cd6431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2342a3a3-1fa5-4f99-8d3f-bf453641d0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ext_train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "303ca632-5f8b-48e7-bbfb-890f6edfe755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['medical_history', 'ground_truth_summary'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0270b6bb-1a10-4788-a72c-7af5b16ced5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class abstractive_summ(Dataset):\n",
    "    def __init__(self, tokenizer, file_path, num_samples, input_length, output_length, print_text=False):         \n",
    "        self.dataset =  pd.read_csv(file_path)\n",
    "        if num_samples:\n",
    "            self.dataset = self.dataset[:num_samples]\n",
    "        self.input_length = input_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.output_length = output_length\n",
    "        self.print_text = print_text\n",
    "  \n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        text = text.replace('\\n','')\n",
    "        text = text.replace('``', '')\n",
    "        text = text.replace('\"', '')\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    \n",
    "    def convert_to_features(self, example_batch):\n",
    "        # Tokenize contexts and questions (as pairs of inputs)\n",
    "        \n",
    "        if self.print_text:\n",
    "            print(\"Input Text: \", self.clean_text(example_batch['medical_history']))\n",
    "#         input_ = self.clean_text(example_batch['text']) + \" </s>\"\n",
    "#         target_ = self.clean_text(example_batch['headline']) + \" </s>\"\n",
    "        \n",
    "        input_ = self.clean_text(example_batch['medical_history'])\n",
    "        target_ = self.clean_text(example_batch['ground_truth_summary'])\n",
    "        \n",
    "        source = self.tokenizer.batch_encode_plus([input_], max_length=self.input_length, \n",
    "                                                     padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        targets = self.tokenizer.batch_encode_plus([target_], max_length=self.output_length, \n",
    "                                                     padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "       \n",
    "        return source, targets\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        source, targets = self.convert_to_features(self.dataset.iloc[index])\n",
    "        \n",
    "        source_ids = source[\"input_ids\"].squeeze()\n",
    "        target_ids = targets[\"input_ids\"].squeeze()\n",
    "\n",
    "        src_mask    = source[\"attention_mask\"].squeeze()\n",
    "        target_mask = targets[\"attention_mask\"].squeeze()\n",
    "\n",
    "        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fed9d4ea-6469-4ff9-8a65-7a5e6ac2fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(tokenizer, type_path, num_samples, args):\n",
    "    file_path = f\"{type_path}_data.csv\"  # train_data.csv, validation_data.csv, test_data.csv\n",
    "    dataset = abstractive_summ(tokenizer=tokenizer, file_path=file_path, num_samples=num_samples, input_length=args.max_input_length, output_length=args.max_output_length)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "130556a5-55ec-4dd5-9c2c-eab6a4c9da29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import pytorch_lightning as pl\n",
    "from datasets import load_metric\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "class T5FineTuner(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super(T5FineTuner, self).__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n",
    "        self.rouge_metric = load_metric('rouge') \n",
    "        \n",
    "        if self.hparams.freeze_embeds:\n",
    "            self.freeze_embeds()\n",
    "        if self.hparams.freeze_encoder:\n",
    "            self.freeze_params(self.model.get_encoder())\n",
    "            assert_all_frozen(self.model.get_encoder())\n",
    "            \n",
    "        self.n_obs = {\n",
    "            \"train\": self.hparams.n_train,\n",
    "            \"validation\": self.hparams.n_val,\n",
    "            \"test\": self.hparams.n_test\n",
    "        }\n",
    "        \n",
    "    def freeze_params(self, model):\n",
    "        for par in model.parameters():\n",
    "            par.requires_grad = False\n",
    "            \n",
    "    def freeze_embeds(self):\n",
    "        try:\n",
    "            self.freeze_params(self.model.shared)\n",
    "            for d in [self.model.encoder, self.model.decoder]:\n",
    "                self.freeze_params(d.embed_tokens)\n",
    "        except AttributeError:\n",
    "            self.freeze_params(self.model.shared)\n",
    "            for d in [self.model.encoder, self.model.decoder]:\n",
    "                self.freeze_params(d.embed_tokens)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, lm_labels=None):\n",
    "        return self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            labels=lm_labels,\n",
    "        )\n",
    "\n",
    "    def _step(self, batch):\n",
    "        lm_labels = batch[\"target_ids\"]\n",
    "        lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        outputs = self(\n",
    "            input_ids=batch[\"source_ids\"],\n",
    "            attention_mask=batch[\"source_mask\"],\n",
    "            lm_labels=lm_labels,\n",
    "            decoder_attention_mask=batch['target_mask']\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "  \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.model.parameters(), lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=self.hparams.total_steps\n",
    "        )\n",
    "        return [optimizer], [{'scheduler': scheduler, 'interval': 'step'}]\n",
    "\n",
    "    def train_dataloader(self):   \n",
    "        train_dataset = get_dataset(self.tokenizer, 'train', self.n_obs['train'], self.hparams)\n",
    "        return DataLoader(train_dataset, batch_size=self.hparams.train_batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        validation_dataset = get_dataset(self.tokenizer, 'validation', self.n_obs['validation'], self.hparams)\n",
    "        return DataLoader(validation_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_dataset = get_dataset(self.tokenizer, 'test', self.n_obs['test'], self.hparams)\n",
    "        return DataLoader(test_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5efb9bd2-25d5-4bc2-a565-8e6e0124c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "args_dict = {\n",
    "    'model_name_or_path': 't5-large',\n",
    "    'tokenizer_name_or_path': 't5-large',\n",
    "    'max_input_length': 512,\n",
    "    'max_output_length': 512,\n",
    "    'freeze_encoder': False,\n",
    "    'freeze_embeds': False,\n",
    "    'learning_rate': 1e-4,\n",
    "    'weight_decay': 0.0,\n",
    "    'adam_epsilon': 1e-8,\n",
    "    'warmup_steps': 0,\n",
    "    'train_batch_size': 16,\n",
    "    'eval_batch_size': 16,\n",
    "    'num_train_epochs': 5,\n",
    "    'n_train': 2000,  #training samples\n",
    "    'n_val': 400,     # validation samples\n",
    "    'n_test': 400,    #test samples\n",
    "    'gradient_accumulation_steps': 16,\n",
    "    'n_gpu': 1,\n",
    "    'early_stop_callback': False,\n",
    "    'fp_16': False,  \n",
    "    'opt_level': 'O1', \n",
    "    'max_grad_norm': 1.0,\n",
    "    'seed': 42,\n",
    "    #'total_steps': (n_train / train_batch_size / gradient_accumulation_steps) * num_train_epochs\n",
    "    'total_steps': 23\n",
    "}\n",
    "args = Namespace(**args_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "841dab8e-f47f-4769-933b-f88d47493d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5FineTuner(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca765761-672a-4c13-ad7c-19c830d259d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=args.num_train_epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b48063a-2f6e-406d-ad7c-b4b7b8180496",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goodone/.local/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:639: Checkpoint directory /home/goodone/Desktop/mlhc/lightning_logs/version_3/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/goodone/.local/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 60.5 M\n",
      "-----------------------------------------------------\n",
      "60.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "60.5 M    Total params\n",
      "242.026   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13ace75c-f15f-4323-802a-b6bfe8af971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # set the model to evaluation mode\n",
    "\n",
    "# Function to generate summary\n",
    "def generate_summary(text, max_length=512):\n",
    "    # Tokenize the input text\n",
    "    inputs =  T5Tokenizer.from_pretrained('t5-small').encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.model.generate(inputs, max_length=max_length, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    \n",
    "    # Decode and return the summary\n",
    "    return T5Tokenizer.from_pretrained('t5-small').decode(summary_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "69aee839-27e5-40e8-a289-c24816d796d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 38-year-old was admitted to the NI 9224** service with close neurological monitoring. she underwent coiling of an aneurysm of the left internal capillary. she was taken to the angiography suite on [**2108-9-24**] where she underwent coiling of an aneurysm.\n"
     ]
    }
   ],
   "source": [
    "test_example = pd.read_csv('test_data.csv')\n",
    "input_text = test_example['medical_history'].iloc[115]\n",
    "summary = generate_summary(input_text)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c74cfa18-5afa-4cbd-82cc-a69c9143b389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[**Known firstname **] [**Known lastname 57383**] is a 38-year-old\n",
      "female who is transferred here from an outside hospital for\n",
      "evaluation of questionable subarachnoid hemorrhage with a\n",
      "negative CT.  She notes that one day prior to admission, at\n",
      "approximately 2:30 p.m. her head and neck \"felt funny.\"  It\n",
      "was not painful, and she had had a gradual onset of\n",
      "increasing pain and then had sudden onset 2 hours later at\n",
      "4:30 in the afternoon of pounding whole head headache and\n",
      "neck pain that she describes as the worst of her life.  She\n",
      "also had some photophobia.  She had no fever or chills.  She\n",
      "had no history of trauma.  Lumbar puncture at an outside\n",
      "hospital showed tube number two 117,000 red blood cells and\n",
      "in tube number three 95,000 red cells.  Total protein was\n",
      "241, glucose was 54, and there was no xanthochromia and no\n",
      "opening pressure was recorded.\n",
      "\n",
      "ALLERGIES:  She has no known drug allergies.\n",
      "\n",
      "MEDICATIONS ON ADMISSION:  None.\n",
      "\n",
      "SOCIAL HISTORY:  She does not drink alcohol and is not a\n",
      "nonsmoker.\n",
      "\n",
      "FAMILY HISTORY:  Shows no aneurysms.\n"
     ]
    }
   ],
   "source": [
    "print(test_example['ground_truth_summary'].iloc[115])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44507686-e7cb-43cf-8cee-a4fd381b3e38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
